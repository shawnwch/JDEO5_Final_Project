{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8c277087-2848-4e4a-92a5-cb47e02db566",
   "metadata": {},
   "source": [
    "1. Data Cleaning for \"Customers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "- ensuring same character length\n",
    "\n",
    "Issues\n",
    "- should duplicated 'customer_unique_id' be removed?\n",
    "- to convert column data type to 'category', the null values have to filled with 'N/A' first before column data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7653237e-de28-484a-aee4-1f3180c05670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_customers_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_customers_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "    \n",
    "    # Step 2: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 3: Convert columns to appropriate data types\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['customer_unique_id'] = df['customer_unique_id'].astype('string')\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype('string')\n",
    "    df['customer_city'] = df['customer_city'].astype('category') \n",
    "    df['customer_state'] = df['customer_state'].astype('category')\n",
    "    \n",
    "    # Step 4: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].str.zfill(5)\n",
    "\n",
    "    # Step 5: Drop duplicates based on the specified subset of columns\n",
    "    df = df.drop_duplicates(subset=['customer_unique_id', 'customer_zip_code_prefix'])\n",
    "\n",
    "    # Step 6: Drop customer id column  based on the specified subset of columns\n",
    "    df = df.drop(columns=\"customer_id\")\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_customers_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22ddc461-900e-4695-8182-57f9bb401374",
   "metadata": {},
   "source": [
    "2. Data Cleaning for \"geolocation\"\n",
    "- removed duplicates, keeping the first occurence\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- should category be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7dcb56-cb3b-4e0e-96a3-2664e6b4dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_geolocation_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_geolocation_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `geolocation_zip_code_prefix`, keeping the first occurrence\n",
    "    df['geolocation_city'] = df['geolocation_city'].apply(lambda x: unidecode(x))\n",
    "    df = df.drop_duplicates(subset='geolocation_zip_code_prefix')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].astype('string')\n",
    "    df['geolocation_lat'] = df['geolocation_lat'].astype('float64')\n",
    "    df['geolocation_lng'] = df['geolocation_lng'].astype('float64')\n",
    "    df['geolocation_city'] = df['geolocation_city'].astype('category')\n",
    "    df['geolocation_state'] = df['geolocation_state'].astype('category')\n",
    "\n",
    "    # Step 5: Normalize `geolocation_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_geolocation_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab317f3b-33d3-4f5e-b3f2-7039d7747b3a",
   "metadata": {},
   "source": [
    "# EXTRACT DATA FOR CITY TABLE\n",
    "\n",
    "# Step 1: Extract distinct city_name and state_code. cleaned_data here is referring to geolocation\n",
    "city = cleaned_data[['geolocation_city', 'geolocation_state']].drop_duplicates().rename(columns={\n",
    "    'geolocation_city': 'city_name',\n",
    "    'geolocation_state': 'state_code'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Step 2: insert auto-increment city_id\n",
    "city.insert(0, 'city_id', range(1, len(city) + 1))\n",
    "\n",
    "# Step 3: Join geolocation and city DataFrames on the city column\n",
    "merged_df = pd.merge(cleaned_data, city, left_on=['geolocation_city','geolocation_state'], right_on=['city_name','state_code']).rename(columns={\n",
    "    'geolocation_lat': 'latitude',\n",
    "    'geolocation_lng': 'longitude'\n",
    "})\n",
    "\n",
    "# Step 4: Drop zipcode and duplicated columns\n",
    "merged_df.drop(columns=['geolocation_zip_code_prefix','geolocation_city','geolocation_state'], inplace=True)\n",
    "\n",
    "# Step 5: Compute the average latitude and longitude, grouped by city_id\n",
    "get_lat_lng = merged_df.groupby(['city_id'], observed=False).agg({\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "})\n",
    "\n",
    "# Step 6: Merge the city with its lat/lng\n",
    "city = pd.merge(city, get_lat_lng, on='city_id').reset_index(drop=True)\n",
    "\n",
    "# Step 7: Correct wrong latitude/longtitude of city (location in Portugal/Spain) to Brazil\n",
    "city.loc[city['city_name'] == 'porto trombetas', ['latitude','longitude']] = [-1.743514558,-52.24416336]\n",
    "city.loc[city['city_name'] == 'santa lucia do piai', ['latitude','longitude']]  = [-29.241292800, -51.021271670]\n",
    "city.loc[city['city_name'] == 'bom retiro da esperanca', ['latitude','longitude']] = [-23.520184363, -48.286817029]\n",
    "city.loc[city['city_name'] == 'areia branca dos assis', ['latitude','longitude']] = [-25.867626304, -49.368047063]\n",
    "city.loc[city['city_name'] == 'ilha dos valadares', ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "#city.loc[city['city_name'] == 'vila nova de campos', ['latitude','longitude']] = [-24.57678608\t-53.79553808] # don't know why not working\n",
    "city.loc[city['city_id'] == 817, ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = 'cleaned/city.csv'\n",
    "city.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80fe2619-c8eb-45ab-a6e0-5e9b771cba90",
   "metadata": {},
   "source": [
    "3. Data Cleaning for \"order_items\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues\n",
    "- The below code did not work, so changed to allow pandas to automatically infer the format by not specifying the format parameter\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'], format='%d/%m/%Y %I:%M:%S %p')\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b665a6-22aa-4945-bc90-0a52c68b56dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_items_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_order_items_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):    \n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['order_item_id'] = df['order_item_id'].astype('int64')\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])\n",
    "    df['price'] = df['price'].apply(Decimal)\n",
    "    df['freight_value'] = df['freight_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_items_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67dba16c-f74e-4fe3-bce7-14a8501950fc",
   "metadata": {},
   "source": [
    "4. Data Cleaning for \"order_payments\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2abe6d4f-262f-4544-ad2d-3cc11f33a8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_payments_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_order_payments_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['payment_sequential'] = df['payment_sequential'].astype('int8')\n",
    "    df['payment_type'] = df['payment_type'].astype('category')\n",
    "    df['payment_installments'] = df['payment_installments'].astype('int8')\n",
    "    df['payment_value'] = df['payment_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_payments_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0086e75-dffc-477b-8be6-be3a79f2d87c",
   "metadata": {},
   "source": [
    "5. Data Cleaning for \"order_reviews\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues \n",
    "- retained review_score as 'int64' instead oa 'category' in case we would like to do mathematical operations such as applying some statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d3f985-fdca-4f3f-95e8-61ad19f582cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_reviews_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_order_reviews_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['review_id'] = df['review_id'].astype('string')\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['review_score'] = df['review_score'].astype('int64')\n",
    "    df['review_comment_title'] = df['review_comment_title'].astype('string')\n",
    "    df['review_comment_message'] = df['review_comment_message'].astype('string')\n",
    "    df['review_creation_date'] = pd.to_datetime(df['review_creation_date'])\n",
    "    df['review_answer_timestamp'] = pd.to_datetime(df['review_answer_timestamp']).dt.date\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_reviews_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a45da81-97df-46d1-b35f-3708319c5249",
   "metadata": {},
   "source": [
    "6. Data Cleaning for \"orders\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues \n",
    "- if set to_datetime first, .fillna later is ok, but if .fillna first, to_datetime later is not ok\n",
    "- however if set to 'category' first, .fillna later is not ok\n",
    "- hence the solution here is to set as 'string' first, then .fillna, then set as 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41df18c8-e472-4bf4-9402-d2ab677b9250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_orders_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_orders_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['order_status'] = df['order_status'].astype('string')\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp']).dt.floor('h')\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at']).dt.floor('h')\n",
    "    df['order_delivered_carrier_date'] = pd.to_datetime(df['order_delivered_carrier_date']).dt.floor('h')\n",
    "    df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date']).dt.floor('h')\n",
    "    df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])\n",
    "\n",
    "    # Step 2: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 5: Convert 'order_status' to category\n",
    "    df['order_status'] = df['order_status'].astype('category')\n",
    "\n",
    "    # Step 6: Add customer_unique_id column to orders dataset, then drop the customer_id column\n",
    "    customers = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_customers_dataset.csv')\n",
    "    df = df.merge(customers[['customer_id', 'customer_unique_id']], left_on='customer_id', right_on='customer_id', how='left')\n",
    "    df = df.drop(columns=['customer_id'])\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_orders_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "793d7521-7eda-4f0f-9c7b-f50a9347dd2f",
   "metadata": {},
   "source": [
    "7. Data Cleaning for \"products\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values in string columns\n",
    "- handling NaN or inf in integer columns\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- issue is that .fillna has to come before .astype('category'), and some columns had null values which cannot be set .astype('int64')\n",
    "- hence, step 3 & 4 separated into str columns and int columns, for filling with 'N/A' or '0' respectively\n",
    "- subsequently, step 5 conversion into 'string', 'category' and 'int64' data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9032ecf3-d67b-4ff0-94d8-b796a7f98c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_products_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_products_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df): \n",
    "    # Step 1: Translate Portugese category names to English\n",
    "    products_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_products_dataset.csv'\n",
    "    translation_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/product_category_name_translation.csv'\n",
    "    products_df = pd.read_csv(products_path)\n",
    "    translation_df = pd.read_csv(translation_path)\n",
    "    \n",
    "    merged_df = products_df.merge(translation_df, on='product_category_name', how='left')\n",
    "    merged_df.insert(1, 'product_category_name_english_merged', merged_df['product_category_name_english'])\n",
    "    merged_dropped_df = merged_df.drop(['product_category_name', 'product_category_name_english'], axis=1)  \n",
    "    df = merged_dropped_df\n",
    "    \n",
    "    # Step 2: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='product_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\" in the string columns:\n",
    "    str_columns = ['product_id', 'product_category_name_english_merged']\n",
    "    df[str_columns] = df[str_columns].fillna('N/A')\n",
    "\n",
    "    # Step 5: Replace NaN or inf with 0 in the integer columns:\n",
    "    int_columns = ['product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "                   'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
    "    df[int_columns] = df[int_columns].replace([np.nan, np.inf, -np.inf], 0)\n",
    "\n",
    "    # Step 6: Convert columns to appropriate data types\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['product_category_name_english_merged'] = df['product_category_name_english_merged'].astype('category')\n",
    "    df['product_name_lenght'] = df['product_name_lenght'].astype('int64')  \n",
    "    df['product_description_lenght'] = df['product_description_lenght'].astype('int64')  \n",
    "    df['product_photos_qty'] = df['product_photos_qty'].astype('int64')  \n",
    "    df['product_weight_g'] = df['product_weight_g'].astype('int64') \n",
    "    df['product_length_cm'] = df['product_length_cm'].astype('int64') \n",
    "    df['product_height_cm'] = df['product_height_cm'].astype('int64') \n",
    "    df['product_width_cm'] = df['product_width_cm'].astype('int64')\n",
    "\n",
    "    # Step 7: Rename columns 'product_name_lenght' and 'product_description_lenght' to the correct spelling of 'length'\n",
    "    df = df.rename(columns={'product_name_lenght': 'product_name_length', 'product_description_lenght' : 'product_description_length'})\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_products_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a0df201-2eb3-4c26-b14e-4fcc467f7f98",
   "metadata": {},
   "source": [
    "8. Data Cleaning for \"sellers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59498bad-4ce8-472a-8632-a1de39ca2fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_sellers_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Olist Data/olist_sellers_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='seller_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].astype('string')\n",
    "    df['seller_city'] = df['seller_city'].astype('category') \n",
    "    df['seller_state'] = df['seller_state'].astype('category')\n",
    "\n",
    "    # Step 5: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_sellers_dataset.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "827a0ef5-21ce-4d1a-a9fb-cc568335e770",
   "metadata": {},
   "source": [
    "9. Connection to and Creation of tables in PostgreSQL using Python\n",
    "- could update your DB username, password and file paths accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49e0f509-b863-4608-b874-0b8c3d04e0b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n",
      "DataFrames loaded successfully.\n",
      "DataFrames written to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connection to and Creation of tables in PostgreSQL using Python\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Database connection string (update with your actual credentials)\n",
    "db_username = 'postgres'  # Replace with your PostgreSQL username\n",
    "db_password = 'password'  # Replace with your PostgreSQL password\n",
    "db_host = 'localhost'        # Replace with your host if not local\n",
    "db_port = 5432               # Default PostgreSQL port\n",
    "db_name = 'OlistDatabase'    # Replace with your database name\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "try:\n",
    "    engine = create_engine(f'postgresql://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "    connection = engine.connect()\n",
    "    print(\"Connection successful.\")\n",
    "    connection.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()  # Stop the script if connection fails\n",
    "\n",
    "# Load DataFrames from CSV files (update paths with correct files)\n",
    "try:\n",
    "    customers = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_customers_dataset.csv')\n",
    "    geolocation = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_geolocation_dataset.csv')\n",
    "    order_items = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_items_dataset.csv')\n",
    "    order_payments = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_payments_dataset.csv')\n",
    "    order_reviews = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_order_reviews_dataset.csv')\n",
    "    orders = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_orders_dataset.csv')\n",
    "    products = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_products_dataset.csv')\n",
    "    sellers = pd.read_csv('/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 8 Final Project/Cleaned Olist Data/cleaned_olist_sellers_dataset.csv')\n",
    "    print(\"DataFrames loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    exit()  # Stop the script if data loading fails\n",
    "\n",
    "# Write DataFrames to PostgreSQL\n",
    "try:\n",
    "    customers.to_sql('customers', engine, if_exists='replace', index=False)\n",
    "    geolocation.to_sql('geolocation', engine, if_exists='replace', index=False)\n",
    "    order_items.to_sql('order_items', engine, if_exists='replace', index=False)\n",
    "    order_payments.to_sql('order_payments', engine, if_exists='replace', index=False)\n",
    "    order_reviews.to_sql('order_reviews', engine, if_exists='replace', index=False)\n",
    "    orders.to_sql('orders', engine, if_exists='replace', index=False)\n",
    "    products.to_sql('products', engine, if_exists='replace', index=False)\n",
    "    sellers.to_sql('sellers', engine, if_exists='replace', index=False)\n",
    "    print(\"DataFrames written to PostgreSQL successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to PostgreSQL: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
