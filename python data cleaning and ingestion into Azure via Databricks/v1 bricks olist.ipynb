{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c878a075-ce15-4ae2-be12-953146c971c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fe93d7-28b1-41f4-b418-b7eccf31ab49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Data Cleaning for \"Customers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "- ensuring same character length\n",
    "\n",
    "Issues\n",
    "- should duplicated 'customer_unique_id' be removed?\n",
    "- to convert column data type to 'category', the null values have to filled with 'N/A' first before column data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582d1592-5cba-4cb3-ad73-d5ab2e629ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to customers table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WriteToSQLServer\").getOrCreate()\n",
    "\n",
    "# Database connection string (update with your actual credentials)\n",
    "db_username = 'USER'  # Replace with your PostgreSQL username\n",
    "db_password = 'PASSWORD'  # Replace with your PostgreSQL password\n",
    "db_host = 'HOST.postgres.database.azure.com'        # Replace with your host if not local\n",
    "db_port = 5432               # Default PostgreSQL port\n",
    "db_name = 'DATABASE'    # Replace with your database name\n",
    "\n",
    "db_url = f'jdbc:postgresql://{db_host}:{db_port}/{db_name}?user={db_username}&password={db_password}&sslmode=require'\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_customers_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/' # hange to your Azure storage URL and folder structure\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "    \n",
    "    # Step 2: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 3: Convert columns to appropriate data types\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['customer_unique_id'] = df['customer_unique_id'].astype('string')\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype('string')\n",
    "    df['customer_city'] = df['customer_city'].astype('category') \n",
    "    df['customer_state'] = df['customer_state'].astype('category')\n",
    "    \n",
    "    # Step 4: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].str.zfill(5)\n",
    "\n",
    "    # Step 5: Drop duplicates based on the specified subset of columns\n",
    "    df = df.drop_duplicates(subset=['customer_unique_id', 'customer_zip_code_prefix'])\n",
    "\n",
    "    # Step 6: Drop customer id column  based on the specified subset of columns\n",
    "    df = df.drop(columns=\"customer_id\")\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "#------------ Cannot write file because I did not set Azure write permission. KIV --------#\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_customers_dataset.csv'\n",
    "#leaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "customers = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'customers'\n",
    "customers.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bff6fa-ac60-4984-93d0-b5e72c91abce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Data Cleaning for \"geolocation\"\n",
    "- removed duplicates, keeping the first occurence\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- should category be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52521604-bdb1-4ac9-93ab-c5fcf75f00e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to geolocation table\n",
      "Cleaned dataset written to city table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "##########################from unidecode import unidecode # databricks does not have unidecode?\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_geolocation_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `geolocation_zip_code_prefix`, keeping the first occurrence\n",
    "    ###############################df['geolocation_city'] = df['geolocation_city'].apply(lambda x: unidecode(x))\n",
    "    df = df.drop_duplicates(subset='geolocation_zip_code_prefix')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].astype('string')\n",
    "    df['geolocation_lat'] = df['geolocation_lat'].astype('float64')\n",
    "    df['geolocation_lng'] = df['geolocation_lng'].astype('float64')\n",
    "    df['geolocation_city'] = df['geolocation_city'].astype('category')\n",
    "    df['geolocation_state'] = df['geolocation_state'].astype('category')\n",
    "\n",
    "    # Step 5: Normalize `geolocation_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/olist_geolocation_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "geolocation = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'geolocation'\n",
    "geolocation.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")\n",
    "\n",
    "#----------------------------------#\n",
    "# Extract city names to City table\n",
    "\n",
    "# Step 1: Extract distinct city_name and state_code. cleaned_data here is referring to geolocation\n",
    "city = cleaned_data[['geolocation_city', 'geolocation_state']].drop_duplicates().rename(columns={\n",
    "    'geolocation_city': 'city_name',\n",
    "    'geolocation_state': 'state_code'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Step 2: insert auto-increment city_id\n",
    "city.insert(0, 'city_id', range(1, len(city) + 1))\n",
    "\n",
    "# Step 3: Join geolocation and city DataFrames on the city column\n",
    "merged_df = pd.merge(cleaned_data, city, left_on=['geolocation_city','geolocation_state'], right_on=['city_name','state_code']).rename(columns={\n",
    "    'geolocation_lat': 'latitude',\n",
    "    'geolocation_lng': 'longitude'\n",
    "})\n",
    "\n",
    "# Step 4: Drop zipcode and duplicated columns\n",
    "merged_df.drop(columns=['geolocation_zip_code_prefix','geolocation_city','geolocation_state'], inplace=True)\n",
    "\n",
    "# Step 5: Compute the average latitude and longitude, grouped by city_id\n",
    "get_lat_lng = merged_df.groupby(['city_id'], observed=False).agg({\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "})\n",
    "\n",
    "# Step 6: Merge the city with its lat/lng\n",
    "city = pd.merge(city, get_lat_lng, on='city_id').reset_index(drop=True)\n",
    "\n",
    "# Step 7: Correct wrong latitude/longtitude of city (location in Portugal/Spain) to Brazil\n",
    "city.loc[city['city_name'] == 'porto trombetas', ['latitude','longitude']] = [-1.743514558,-52.24416336]\n",
    "city.loc[city['city_name'] == 'santa lucia do piai', ['latitude','longitude']]  = [-29.241292800, -51.021271670]\n",
    "city.loc[city['city_name'] == 'bom retiro da esperanca', ['latitude','longitude']] = [-23.520184363, -48.286817029]\n",
    "city.loc[city['city_name'] == 'areia branca dos assis', ['latitude','longitude']] = [-25.867626304, -49.368047063]\n",
    "city.loc[city['city_name'] == 'ilha dos valadares', ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "#city.loc[city['city_name'] == 'vila nova de campos', ['latitude','longitude']] = [-24.57678608\t-53.79553808] # don't know why not working\n",
    "city.loc[city['city_id'] == 817, ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = 'cleaned/city.csv'\n",
    "#city.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "#---------------------------------#\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "city = spark.createDataFrame(city)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'city'\n",
    "city.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef3b0b9-f33a-4983-9865-b60d5c714841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Data Cleaning for \"order_items\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues\n",
    "- The below code did not work, so changed to allow pandas to automatically infer the format by not specifying the format parameter\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'], format='%d/%m/%Y %I:%M:%S %p')\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8770dd-9fb8-4e12-b9a2-ac973ed39b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  precision should be between 1 and 38\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to seller table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_order_items_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):    \n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['order_item_id'] = df['order_item_id'].astype('int64')\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])\n",
    "    df['price'] = df['price'].apply(Decimal)\n",
    "    df['freight_value'] = df['freight_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_order_items_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "seller = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'seller'\n",
    "seller.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097f7444-3d18-420d-98a4-cd78d48b1021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Data Cleaning for \"order_payments\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d9a961-ac6d-4de8-b572-18f041fde0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  precision should be between 1 and 38\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to order_payments table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_order_payments_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['payment_sequential'] = df['payment_sequential'].astype('int8')\n",
    "    df['payment_type'] = df['payment_type'].astype('category')\n",
    "    df['payment_installments'] = df['payment_installments'].astype('int8')\n",
    "    df['payment_value'] = df['payment_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_order_payments_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_payments = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'order_payments'\n",
    "order_payments.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fc3849-00ce-4ac5-9be8-9a43366a0dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Data Cleaning for \"order_reviews\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues \n",
    "- retained review_score as 'int64' instead oa 'category' in case we would like to do mathematical operations such as applying some statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c437a4fd-de2b-432f-bd11-da0270a48452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to order_reviews table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_order_reviews_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['review_id'] = df['review_id'].astype('string')\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['review_score'] = df['review_score'].astype('int64')\n",
    "    df['review_comment_title'] = df['review_comment_title'].astype('string')\n",
    "    df['review_comment_message'] = df['review_comment_message'].astype('string')\n",
    "    df['review_creation_date'] = pd.to_datetime(df['review_creation_date'])\n",
    "    df['review_answer_timestamp'] = pd.to_datetime(df['review_answer_timestamp']).dt.date\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_order_reviews_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_reviews = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'order_reviews'\n",
    "order_reviews.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a41aa09-6952-4237-b609-f9ab7ef1f8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Data Cleaning for \"orders\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues \n",
    "- if set to_datetime first, .fillna later is ok, but if .fillna first, to_datetime later is not ok\n",
    "- however if set to 'category' first, .fillna later is not ok\n",
    "- hence the solution here is to set as 'string' first, then .fillna, then set as 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5c392b-9af3-445b-9a78-f85b89b466ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Cannot specify a mask or a size when passing an object that is converted with the __arrow_array__ protocol.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to orders table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_orders_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['order_status'] = df['order_status'].astype('string')\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp']).dt.floor('h')\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at']).dt.floor('h')\n",
    "    df['order_delivered_carrier_date'] = pd.to_datetime(df['order_delivered_carrier_date']).dt.floor('h')\n",
    "    df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date']).dt.floor('h')\n",
    "    df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])\n",
    "\n",
    "    # Step 2: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\":\n",
    "    ###############################################df = df.fillna('N/A')\n",
    "\n",
    "    # Step 5: Convert 'order_status' to category\n",
    "    df['order_status'] = df['order_status'].astype('category')\n",
    "\n",
    "    # Step 6: Add customer_unique_id column to orders dataset, then drop the customer_id column\n",
    "    customers = pd.read_csv(azurl + 'olist_customers_dataset.csv')\n",
    "    df = df.merge(customers[['customer_id', 'customer_unique_id']], left_on='customer_id', right_on='customer_id', how='left')\n",
    "    df = df.drop(columns=['customer_id'])\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_orders_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "orders = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'orders'\n",
    "orders.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df87d5b6-2f59-4903-8303-5ed8ec2ba4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Data Cleaning for \"products\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values in string columns\n",
    "- handling NaN or inf in integer columns\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- issue is that .fillna has to come before .astype('category'), and some columns had null values which cannot be set .astype('int64')\n",
    "- hence, step 3 & 4 separated into str columns and int columns, for filling with 'N/A' or '0' respectively\n",
    "- subsequently, step 5 conversion into 'string', 'category' and 'int64' data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094e069c-62f6-4fa6-942b-64469b2111dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to products table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_products_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df): \n",
    "    # Step 1: Translate Portugese category names to English\n",
    "    products_path = azurl + 'olist_products_dataset.csv'\n",
    "    translation_path = azurl + 'product_category_name_translation.csv'\n",
    "    products_df = pd.read_csv(products_path)\n",
    "    translation_df = pd.read_csv(translation_path)\n",
    "    \n",
    "    merged_df = products_df.merge(translation_df, on='product_category_name', how='left')\n",
    "    merged_df.insert(1, 'product_category_name_english_merged', merged_df['product_category_name_english'])\n",
    "    merged_dropped_df = merged_df.drop(['product_category_name', 'product_category_name_english'], axis=1)  \n",
    "    df = merged_dropped_df\n",
    "    \n",
    "    # Step 2: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='product_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\" in the string columns:\n",
    "    str_columns = ['product_id', 'product_category_name_english_merged']\n",
    "    df[str_columns] = df[str_columns].fillna('N/A')\n",
    "\n",
    "    # Step 5: Replace NaN or inf with 0 in the integer columns:\n",
    "    int_columns = ['product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "                   'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
    "    df[int_columns] = df[int_columns].replace([np.nan, np.inf, -np.inf], 0)\n",
    "\n",
    "    # Step 6: Convert columns to appropriate data types\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['product_category_name_english_merged'] = df['product_category_name_english_merged'].astype('category')\n",
    "    df['product_name_lenght'] = df['product_name_lenght'].astype('int64')  \n",
    "    df['product_description_lenght'] = df['product_description_lenght'].astype('int64')  \n",
    "    df['product_photos_qty'] = df['product_photos_qty'].astype('int64')  \n",
    "    df['product_weight_g'] = df['product_weight_g'].astype('int64') \n",
    "    df['product_length_cm'] = df['product_length_cm'].astype('int64') \n",
    "    df['product_height_cm'] = df['product_height_cm'].astype('int64') \n",
    "    df['product_width_cm'] = df['product_width_cm'].astype('int64')\n",
    "\n",
    "    # Step 7: Rename columns 'product_name_lenght' and 'product_description_lenght' to the correct spelling of 'length'\n",
    "    df = df.rename(columns={'product_category_name_english_merged':'product_category','product_name_lenght': 'product_name_length', 'product_description_lenght' : 'product_description_length'})\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_products_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "products = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'products'\n",
    "products.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e13f6d8-ef50-40c9-a160-bc7687ae129b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Data Cleaning for \"sellers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38e3b38-d9c8-42cd-9bca-bbd17bb93775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to sellers table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'olist_sellers_dataset.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='seller_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].astype('string')\n",
    "    df['seller_city'] = df['seller_city'].astype('category') \n",
    "    df['seller_state'] = df['seller_state'].astype('category')\n",
    "\n",
    "    # Step 5: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "#cleaned_file_path = azurl + 'cleaned/cleaned_olist_sellers_dataset.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "sellers = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'sellers'\n",
    "sellers.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd073e43-315a-498b-a193-af197508b2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47313f65-4160-41c8-baa3-c870309fb236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to states table\n"
     ]
    }
   ],
   "source": [
    "# State names look up table\n",
    "# Data from other Kaggler dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "########################from unidecode import unidecode  # For normalising accented strings\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'misc/states.csv'\n",
    "azurl = 'https://olistblobs.blob.core.windows.net/olist/olist/'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "\n",
    "    df = df[['UF', 'State']]  # Keep state codes and names only\n",
    "    df = df.rename(columns={'UF': 'code', 'State' : 'state'})  # Rename UF and State to code and state respectively\n",
    "    \n",
    "    # Normalise accented strings to normal alphabet strings\n",
    "    #####################df['state'] = df['state'].apply(lambda x: unidecode(x))\n",
    "    \n",
    "    # Copy converted proper name to title case \n",
    "    df['state'] = df['state'].str.title()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "#cleaned_file_path = azurl 'cleaned/states_lookup.csv'\n",
    "#cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "#print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "states = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'states'\n",
    "states.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7050ff82-d4bf-47cd-89ed-216e783592bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks version",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
