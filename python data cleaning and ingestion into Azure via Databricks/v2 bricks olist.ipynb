{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c878a075-ce15-4ae2-be12-953146c971c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7dec38fd-63cb-4465-808b-ac1eff520781/lib/python3.11/site-packages (1.3.8)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This cell can be used for testing or pip install. To remove at final version\n",
    "\n",
    "%pip install unidecode\n",
    "#from unidecode import unidecode\n",
    "#print(unidecode(\"caf√©\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc450b4-2d91-44c9-97de-94147e1a19d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep cell empty to use as start-run cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419b986a-f8a4-4fb3-8261-24d280e31ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to import libraries, set up URL etc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from unidecode import unidecode\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WriteToSQLServer\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\").getOrCreate() \n",
    "\n",
    "# Database connection string (update with your actual credentials)\n",
    "db_username = 'USER'  # Replace with your PostgreSQL username\n",
    "db_password = 'PASSWORD'  # Replace with your PostgreSQL password\n",
    "db_host = 'HOST.postgres.database.azure.com'        # Replace with your host if not local\n",
    "db_port = 5432               # Default PostgreSQL port\n",
    "db_name = 'DATABASE'    # Replace with your database name\n",
    "\n",
    "db_url = f'jdbc:postgresql://{db_host}:{db_port}/{db_name}?user={db_username}&password={db_password}&sslmode=require'\n",
    "\n",
    "# Azure Blob storage access setup for writing CSV\n",
    "storage_account_name = \"<Your Storage Account Name>\" \n",
    "container_name = \"<Your Container Name>\" \n",
    "storage_account_access_key = \"<Your Access Key>\"\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_access_key\n",
    ")\n",
    "\n",
    "# Azure storage URL and folder where olist dataset is stored \n",
    "folder_name = \"olist\"\n",
    "azurl = f\"https://{storage_account_name}.blob.core.windows.net/{container_name}/{folder_name}/\"\n",
    "\n",
    "# Folder to store cleaned CSV\n",
    "cleaned_folder = \"olist/cleaned/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fe93d7-28b1-41f4-b418-b7eccf31ab49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Data Cleaning for \"Customers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "- ensuring same character length\n",
    "\n",
    "Issues\n",
    "- should duplicated 'customer_unique_id' be removed?\n",
    "- to convert column data type to 'category', the null values have to filled with 'N/A' first before column data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582d1592-5cba-4cb3-ad73-d5ab2e629ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to customers table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_customers_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "    \n",
    "    # Step 2: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 3: Convert columns to appropriate data types\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['customer_unique_id'] = df['customer_unique_id'].astype('string')\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype('string')\n",
    "    df['customer_city'] = df['customer_city'].astype('category') \n",
    "    df['customer_state'] = df['customer_state'].astype('category')\n",
    "    \n",
    "    # Step 4: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].str.zfill(5)\n",
    "\n",
    "    # Step 5: Drop duplicates based on the specified subset of columns\n",
    "    df = df.drop_duplicates(subset=['customer_unique_id', 'customer_zip_code_prefix'])\n",
    "\n",
    "    # Step 6: Drop customer id column  based on the specified subset of columns\n",
    "    df = df.drop(columns=\"customer_id\")\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "customers = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_customers_dataset.csv\"\n",
    "customers.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'customers'\n",
    "customers.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bff6fa-ac60-4984-93d0-b5e72c91abce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Data Cleaning for \"geolocation\"\n",
    "- removed duplicates, keeping the first occurence\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- should category be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52521604-bdb1-4ac9-93ab-c5fcf75f00e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to geolocation table\n",
      "Cleaned dataset written to city table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_geolocation_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `geolocation_zip_code_prefix`, keeping the first occurrence\n",
    "    df['geolocation_city'] = df['geolocation_city'].apply(lambda x: unidecode(x))\n",
    "    df = df.drop_duplicates(subset='geolocation_zip_code_prefix')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].astype('string')\n",
    "    df['geolocation_lat'] = df['geolocation_lat'].astype('float64')\n",
    "    df['geolocation_lng'] = df['geolocation_lng'].astype('float64')\n",
    "    df['geolocation_city'] = df['geolocation_city'].astype('category')\n",
    "    df['geolocation_state'] = df['geolocation_state'].astype('category')\n",
    "\n",
    "    # Step 5: Normalize `geolocation_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "geolocation = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_geolocation_dataset.csv\"\n",
    "geolocation.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'geolocation'\n",
    "geolocation.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "# Extract city names to City table\n",
    "\n",
    "# Step 1: Extract distinct city_name and state_code. cleaned_data here is referring to geolocation\n",
    "city = cleaned_data[['geolocation_city', 'geolocation_state']].drop_duplicates().rename(columns={\n",
    "    'geolocation_city': 'city_name',\n",
    "    'geolocation_state': 'state_code'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Step 2: insert auto-increment city_id\n",
    "city.insert(0, 'city_id', range(1, len(city) + 1))\n",
    "\n",
    "# Step 3: Join geolocation and city DataFrames on the city column\n",
    "merged_df = pd.merge(cleaned_data, city, left_on=['geolocation_city','geolocation_state'], right_on=['city_name','state_code']).rename(columns={\n",
    "    'geolocation_lat': 'latitude',\n",
    "    'geolocation_lng': 'longitude'\n",
    "})\n",
    "\n",
    "# Step 4: Drop zipcode and duplicated columns\n",
    "merged_df.drop(columns=['geolocation_zip_code_prefix','geolocation_city','geolocation_state'], inplace=True)\n",
    "\n",
    "# Step 5: Compute the average latitude and longitude, grouped by city_id\n",
    "get_lat_lng = merged_df.groupby(['city_id'], observed=False).agg({\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "})\n",
    "\n",
    "# Step 6: Merge the city with its lat/lng\n",
    "city = pd.merge(city, get_lat_lng, on='city_id').reset_index(drop=True)\n",
    "\n",
    "# Step 7: Correct wrong latitude/longtitude of city (location in Portugal/Spain) to Brazil\n",
    "city.loc[city['city_name'] == 'porto trombetas', ['latitude','longitude']] = [-1.743514558,-52.24416336]\n",
    "city.loc[city['city_name'] == 'santa lucia do piai', ['latitude','longitude']]  = [-29.241292800, -51.021271670]\n",
    "city.loc[city['city_name'] == 'bom retiro da esperanca', ['latitude','longitude']] = [-23.520184363, -48.286817029]\n",
    "city.loc[city['city_name'] == 'areia branca dos assis', ['latitude','longitude']] = [-25.867626304, -49.368047063]\n",
    "city.loc[city['city_name'] == 'ilha dos valadares', ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "#city.loc[city['city_name'] == 'vila nova de campos', ['latitude','longitude']] = [-24.57678608\t-53.79553808] # don't know why not working\n",
    "city.loc[city['city_id'] == 817, ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "city = spark.createDataFrame(city)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_city_dataset.csv\"\n",
    "city.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'city'\n",
    "city.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef3b0b9-f33a-4983-9865-b60d5c714841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Data Cleaning for \"order_items\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues\n",
    "- The below code did not work, so changed to allow pandas to automatically infer the format by not specifying the format parameter\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'], format='%d/%m/%Y %I:%M:%S %p')\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8770dd-9fb8-4e12-b9a2-ac973ed39b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to order_items table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_order_items_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):    \n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['order_item_id'] = df['order_item_id'].astype('int64')\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])\n",
    "    df['price'] = df['price'].apply(Decimal)\n",
    "    df['freight_value'] = df['freight_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_items = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_order_items_dataset.csv\"\n",
    "order_items.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'order_items'\n",
    "order_items.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097f7444-3d18-420d-98a4-cd78d48b1021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Data Cleaning for \"order_payments\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d9a961-ac6d-4de8-b572-18f041fde0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to order_payments table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_order_payments_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['payment_sequential'] = df['payment_sequential'].astype('int8')\n",
    "    df['payment_type'] = df['payment_type'].astype('category')\n",
    "    df['payment_installments'] = df['payment_installments'].astype('int8')\n",
    "    df['payment_value'] = df['payment_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_payments = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_order_payments_dataset.csv\"\n",
    "order_payments.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'order_payments'\n",
    "order_payments.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fc3849-00ce-4ac5-9be8-9a43366a0dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Data Cleaning for \"order_reviews\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues \n",
    "- retained review_score as 'int64' instead oa 'category' in case we would like to do mathematical operations such as applying some statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c437a4fd-de2b-432f-bd11-da0270a48452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to order_reviews table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_order_reviews_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['review_id'] = df['review_id'].astype('string')\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['review_score'] = df['review_score'].astype('int64')\n",
    "    df['review_comment_title'] = df['review_comment_title'].astype('string')\n",
    "    df['review_comment_message'] = df['review_comment_message'].astype('string')\n",
    "    df['review_creation_date'] = pd.to_datetime(df['review_creation_date'])\n",
    "    df['review_answer_timestamp'] = pd.to_datetime(df['review_answer_timestamp']).dt.date\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_reviews = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_order_reviews_dataset.csv\"\n",
    "order_reviews.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'order_reviews'\n",
    "order_reviews.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a41aa09-6952-4237-b609-f9ab7ef1f8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Data Cleaning for \"orders\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues \n",
    "- if set to_datetime first, .fillna later is ok, but if .fillna first, to_datetime later is not ok\n",
    "- however if set to 'category' first, .fillna later is not ok\n",
    "- hence the solution here is to set as 'string' first, then .fillna, then set as 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5c392b-9af3-445b-9a78-f85b89b466ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to orders table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_orders_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['order_status'] = df['order_status'].astype('string')\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp']).dt.floor('h')\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at']).dt.floor('h')\n",
    "    df['order_delivered_carrier_date'] = pd.to_datetime(df['order_delivered_carrier_date']).dt.floor('h')\n",
    "    df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date']).dt.floor('h')\n",
    "    df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])\n",
    "\n",
    "    # Step 2: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\":\n",
    "    ######################################df = df.fillna('N/A')\n",
    "\n",
    "    # Step 5: Convert 'order_status' to category\n",
    "    df['order_status'] = df['order_status'].astype('category')\n",
    "\n",
    "    # Step 6: Add customer_unique_id column to orders dataset, then drop the customer_id column\n",
    "    customers = pd.read_csv(azurl + 'olist_customers_dataset.csv')\n",
    "    df = df.merge(customers[['customer_id', 'customer_unique_id']], left_on='customer_id', right_on='customer_id', how='left')\n",
    "    df = df.drop(columns=['customer_id'])\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "orders = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_orders_dataset.csv\"\n",
    "orders.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'orders'\n",
    "orders.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df87d5b6-2f59-4903-8303-5ed8ec2ba4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Data Cleaning for \"products\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values in string columns\n",
    "- handling NaN or inf in integer columns\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- issue is that .fillna has to come before .astype('category'), and some columns had null values which cannot be set .astype('int64')\n",
    "- hence, step 3 & 4 separated into str columns and int columns, for filling with 'N/A' or '0' respectively\n",
    "- subsequently, step 5 conversion into 'string', 'category' and 'int64' data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094e069c-62f6-4fa6-942b-64469b2111dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to products table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_products_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df): \n",
    "    # Step 1: Translate Portugese category names to English\n",
    "    products_path = azurl + 'olist_products_dataset.csv'\n",
    "    translation_path = azurl + 'product_category_name_translation.csv'\n",
    "    products_df = pd.read_csv(products_path)\n",
    "    translation_df = pd.read_csv(translation_path)\n",
    "    \n",
    "    merged_df = products_df.merge(translation_df, on='product_category_name', how='left')\n",
    "    merged_df.insert(1, 'product_category_name_english_merged', merged_df['product_category_name_english'])\n",
    "    merged_dropped_df = merged_df.drop(['product_category_name', 'product_category_name_english'], axis=1)  \n",
    "    df = merged_dropped_df\n",
    "    \n",
    "    # Step 2: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='product_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\" in the string columns:\n",
    "    str_columns = ['product_id', 'product_category_name_english_merged']\n",
    "    df[str_columns] = df[str_columns].fillna('N/A')\n",
    "\n",
    "    # Step 5: Replace NaN or inf with 0 in the integer columns:\n",
    "    int_columns = ['product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "                   'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
    "    df[int_columns] = df[int_columns].replace([np.nan, np.inf, -np.inf], 0)\n",
    "\n",
    "    # Step 6: Convert columns to appropriate data types\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['product_category_name_english_merged'] = df['product_category_name_english_merged'].astype('category')\n",
    "    df['product_name_lenght'] = df['product_name_lenght'].astype('int64')  \n",
    "    df['product_description_lenght'] = df['product_description_lenght'].astype('int64')  \n",
    "    df['product_photos_qty'] = df['product_photos_qty'].astype('int64')  \n",
    "    df['product_weight_g'] = df['product_weight_g'].astype('int64') \n",
    "    df['product_length_cm'] = df['product_length_cm'].astype('int64') \n",
    "    df['product_height_cm'] = df['product_height_cm'].astype('int64') \n",
    "    df['product_width_cm'] = df['product_width_cm'].astype('int64')\n",
    "\n",
    "    # Step 7: Rename columns 'product_name_lenght' and 'product_description_lenght' to the correct spelling of 'length'\n",
    "    df = df.rename(columns={'product_category_name_english_merged':'product_category','product_name_lenght': 'product_name_length', 'product_description_lenght' : 'product_description_length'})\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "products = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_products_dataset.csv\"\n",
    "products.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'products'\n",
    "products.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e13f6d8-ef50-40c9-a160-bc7687ae129b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Data Cleaning for \"sellers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38e3b38-d9c8-42cd-9bca-bbd17bb93775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to sellers table\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_sellers_dataset.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='seller_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].astype('string')\n",
    "    df['seller_city'] = df['seller_city'].astype('category') \n",
    "    df['seller_state'] = df['seller_state'].astype('category')\n",
    "\n",
    "    # Step 5: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "sellers = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_sellers_dataset.csv\"\n",
    "sellers.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'sellers'\n",
    "sellers.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47313f65-4160-41c8-baa3-c870309fb236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to states table\n"
     ]
    }
   ],
   "source": [
    "# State names look up table\n",
    "# Data from other Kaggler dataset\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'misc/states.csv'\n",
    "file = azurl + file_name\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "\n",
    "    df = df[['UF', 'State']]  # Keep state codes and names only\n",
    "    df = df.rename(columns={'UF': 'code', 'State' : 'state'})  # Rename UF and State to code and state respectively\n",
    "    \n",
    "    # Normalise accented strings to normal alphabet strings\n",
    "    df['state'] = df['state'].apply(lambda x: unidecode(x))\n",
    "    \n",
    "    # Copy converted proper name to title case \n",
    "    df['state'] = df['state'].str.title()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "states = spark.createDataFrame(cleaned_data)\n",
    "\n",
    "# Write cleaned CSV to Blob Storage cleaned folder\n",
    "cleaned_file = \"states_lookup.csv\"\n",
    "states.write.format(\"csv\").mode(\"overwrite\").save(f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{cleaned_folder}{cleaned_file}\")\n",
    "\n",
    "# Write to Azure Postgres\n",
    "table = 'states'\n",
    "states.write.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", table).mode('overwrite').save()\n",
    "print(f\"Cleaned dataset written to {table} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7050ff82-d4bf-47cd-89ed-216e783592bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "v2 bricks olist",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
