{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c878a075-ce15-4ae2-be12-953146c971c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Obtaining dependency information for unidecode from https://files.pythonhosted.org/packages/84/b7/6ec57841fb67c98f52fc8e4a2d96df60059637cba077edc569a302a8ffc7/Unidecode-1.3.8-py3-none-any.whl.metadata\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: azure-storage-blob in /databricks/python3/lib/python3.11/site-packages (12.19.1)\n",
      "Collecting sqlalchemy\n",
      "  Obtaining dependency information for sqlalchemy from https://files.pythonhosted.org/packages/57/4f/e1db9475f940f1c54c365ed02d4f6390f884fc95a6a4022ece7725956664/SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Obtaining dependency information for psycopg2-binary from https://files.pythonhosted.org/packages/5d/f1/09f45ac25e704ac954862581f9f9ae21303cc5ded3d0b775532b407f0e90/psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (41.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (4.10.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (0.6.1)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Obtaining dependency information for greenlet!=0.4.17 from https://files.pythonhosted.org/packages/f7/4b/1c9695aa24f808e156c8f4813f685d975ca73c000c2a5056c514c64980f6/greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/602.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.4/602.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode, psycopg2-binary, greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.1.1 psycopg2-binary-2.9.10 sqlalchemy-2.0.37 unidecode-1.3.8\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This cell can be used for testing or pip install. To remove at final version\n",
    "\n",
    "%pip install unidecode azure-storage-blob sqlalchemy psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc450b4-2d91-44c9-97de-94147e1a19d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep cell empty to use as start-run cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419b986a-f8a4-4fb3-8261-24d280e31ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to import libraries, connect to Azure postgreSQL flexible server & blob storage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from unidecode import unidecode\n",
    "from sqlalchemy import create_engine\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "from io import StringIO\n",
    "\n",
    "# Database connection string \n",
    "db_username = 'azurepg'  # Replace with your PostgreSQL username\n",
    "db_password = 'postgres#1'  # Replace with your PostgreSQL password\n",
    "db_host = 'azurepg.postgres.database.azure.com'        # Replace with your host if not local\n",
    "db_port = 5432               # Default PostgreSQL port\n",
    "db_name = 'postgres'    # Replace with your database name\n",
    "\n",
    "# Create connection to Azure PostgreSQL server\n",
    "conn_str = f\"postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Azure Blob Storage connection details\n",
    "account_name = \"olistblobs\"\n",
    "account_key = \"eZaLN3sA0Ze1VEXPaeri6nUQLfSGGOk7guVZ0JIy26XYBhh+aEq/1Bdlpxi4SI+7XXIEe+KP97eX+AStKfyeCQ==\"\n",
    "container_name = \"olist\"\n",
    "folder = \"olist/\"\n",
    "cleaned_folder = \"cleaned/\"\n",
    "\n",
    "# Create a connection to Azure blob storage and container   \n",
    "container_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key).get_container_client(container_name) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fe93d7-28b1-41f4-b418-b7eccf31ab49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Data Cleaning for \"Customers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "- ensuring same character length\n",
    "\n",
    "Issues\n",
    "- should duplicated 'customer_unique_id' be removed?\n",
    "- to convert column data type to 'category', the null values have to filled with 'N/A' first before column data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582d1592-5cba-4cb3-ad73-d5ab2e629ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     customer_unique_id customer_zip_code_prefix  \\\n",
      "0      861eff4711a542e4b93843c6dd7febb0                    14409   \n",
      "1      290c77bc529b7ac935b93aa66c333dc3                    09790   \n",
      "2      060e732b5b29e8181a18229c7b0b2b5e                    01151   \n",
      "3      259dac757896d24d7702b9acbbff3f3c                    08775   \n",
      "4      345ecd01c38d18a9036ed96c73b8d066                    13056   \n",
      "...                                 ...                      ...   \n",
      "99436  1a29b476fee25c95fbafc67c5ac95cf8                    03937   \n",
      "99437  d52a67c98be1cf6a5c84435bd38d095d                    06764   \n",
      "99438  e9f50caf99f032f0bf3c55141f019d99                    60115   \n",
      "99439  73c2643a0a458b49f58cea58833b192e                    92120   \n",
      "99440  84732c5050c01db9b23e19ba39899398                    06703   \n",
      "\n",
      "               customer_city customer_state  \n",
      "0                     Franca             SP  \n",
      "1      Sao Bernardo Do Campo             SP  \n",
      "2                  Sao Paulo             SP  \n",
      "3            Mogi Das Cruzes             SP  \n",
      "4                   Campinas             SP  \n",
      "...                      ...            ...  \n",
      "99436              Sao Paulo             SP  \n",
      "99437        Taboao Da Serra             SP  \n",
      "99438              Fortaleza             CE  \n",
      "99439                 Canoas             RS  \n",
      "99440                  Cotia             SP  \n",
      "\n",
      "[96096 rows x 4 columns]\n",
      "Cleaned dataset cleaned_olist_customers_dataset.csv written successfully!\n",
      "Data written to customers table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_customers_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 2: Increase readibility of the categorical data\n",
    "    df.loc[:,'customer_city'] = df['customer_city'].str.title()\n",
    "    \n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['customer_unique_id'] = df['customer_unique_id'].astype('string')\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype('string')\n",
    "    df['customer_city'] = df['customer_city'].astype('category') \n",
    "    df['customer_state'] = df['customer_state'].astype('category')\n",
    "    \n",
    "    # Step 5: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].str.zfill(5)\n",
    "\n",
    "    # Step 6: Drop customer id column  based on the specified subset of columns\n",
    "    df = df.drop(columns=\"customer_id\")\n",
    "\n",
    "    # Step 7: Drop duplicates based on the specified subset of columns\n",
    "    df = df.drop_duplicates(subset='customer_unique_id')\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_customers_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'customers'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bff6fa-ac60-4984-93d0-b5e72c91abce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Data Cleaning for \"geolocation\"\n",
    "- removed duplicates, keeping the first occurence\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- should category be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52521604-bdb1-4ac9-93ab-c5fcf75f00e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/1289/command-2477397622210495-1259402318:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'geolocation_city'] = df['geolocation_city'].str.title()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_geolocation_dataset.csv written successfully!\n",
      "Data written to geolocation table in Azure PostgreSQL\n",
      "Cleaned dataset cleaned_olist_city_dataset.csv written successfully!\n",
      "Data written to city table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_geolocation_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `geolocation_zip_code_prefix`, keeping the first occurrence\n",
    "    df['geolocation_city'] = df['geolocation_city'].apply(lambda x: unidecode(x))\n",
    "    df = df.drop_duplicates(subset='geolocation_zip_code_prefix')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Increase readibility of the categorical data\n",
    "    df.loc[:, 'geolocation_city'] = df['geolocation_city'].str.title()\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 5: Convert columns to appropriate data types\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].astype('string')\n",
    "    df['geolocation_lat'] = df['geolocation_lat'].astype('float64')\n",
    "    df['geolocation_lng'] = df['geolocation_lng'].astype('float64')\n",
    "    df['geolocation_city'] = df['geolocation_city'].astype('category')\n",
    "    df['geolocation_state'] = df['geolocation_state'].astype('category')\n",
    "\n",
    "    # Step 6: Normalize `geolocation_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_geolocation_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'geolocation'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n",
    "\n",
    "#----------------------------------#\n",
    "# Extract city names to City table\n",
    "\n",
    "# Step 1: Extract distinct city_name and state_code. cleaned_data here is referring to geolocation\n",
    "city = cleaned_data[['geolocation_city', 'geolocation_state']].drop_duplicates().rename(columns={\n",
    "    'geolocation_city': 'city_name',\n",
    "    'geolocation_state': 'state_code'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Step 2: insert auto-increment city_id\n",
    "city.insert(0, 'city_id', range(1, len(city) + 1))\n",
    "\n",
    "# Step 3: Join geolocation and city DataFrames on the city column\n",
    "merged_df = pd.merge(cleaned_data, city, left_on=['geolocation_city','geolocation_state'], right_on=['city_name','state_code']).rename(columns={\n",
    "    'geolocation_lat': 'latitude',\n",
    "    'geolocation_lng': 'longitude'\n",
    "})\n",
    "\n",
    "# Step 4: Drop zipcode and duplicated columns\n",
    "merged_df.drop(columns=['geolocation_zip_code_prefix','geolocation_city','geolocation_state'], inplace=True)\n",
    "\n",
    "# Step 5: Compute the average latitude and longitude, grouped by city_id\n",
    "get_lat_lng = merged_df.groupby(['city_id'], observed=False).agg({\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "})\n",
    "\n",
    "# Step 6: Merge the city with its lat/lng\n",
    "city = pd.merge(city, get_lat_lng, on='city_id').reset_index(drop=True)\n",
    "\n",
    "# Step 7: Correct wrong latitude/longtitude of city (location in Portugal/Spain) to Brazil\n",
    "city.loc[city['city_name'] == 'porto trombetas', ['latitude','longitude']] = [-1.743514558,-52.24416336]\n",
    "city.loc[city['city_name'] == 'santa lucia do piai', ['latitude','longitude']]  = [-29.241292800, -51.021271670]\n",
    "city.loc[city['city_name'] == 'bom retiro da esperanca', ['latitude','longitude']] = [-23.520184363, -48.286817029]\n",
    "city.loc[city['city_name'] == 'areia branca dos assis', ['latitude','longitude']] = [-25.867626304, -49.368047063]\n",
    "city.loc[city['city_name'] == 'ilha dos valadares', ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "#city.loc[city['city_name'] == 'vila nova de campos', ['latitude','longitude']] = [-24.57678608\t-53.79553808] # don't know why not working\n",
    "city.loc[city['city_id'] == 817, ['latitude','longitude']] = [-25.533502571, -48.508189284]\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_city_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'city'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef3b0b9-f33a-4983-9865-b60d5c714841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Data Cleaning for \"order_items\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues\n",
    "- The below code did not work, so changed to allow pandas to automatically infer the format by not specifying the format parameter\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'], format='%d/%m/%Y %I:%M:%S %p')\n",
    "    - df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8770dd-9fb8-4e12-b9a2-ac973ed39b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_order_items_dataset.csv written successfully!\n",
      "Data written to order_items table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_order_items_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):    \n",
    "    # Step 1: Remove duplicates in `order_id` + 'order_item_id' if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['order_id','order_item_id'])\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['order_item_id'] = df['order_item_id'].astype('int64')\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])\n",
    "    df['price'] = df['price'].apply(Decimal)\n",
    "    df['freight_value'] = df['freight_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_order_items_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'order_items'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097f7444-3d18-420d-98a4-cd78d48b1021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Data Cleaning for \"order_payments\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d9a961-ac6d-4de8-b572-18f041fde0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/1289/command-2477397622210499-1323081223:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'payment_type'] = df['payment_type'].str.replace('_',' ').str.title()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_order_payments_dataset.csv written successfully!\n",
      "Data written to order_payments table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_order_payments_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id`+'pyament_sequential' if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['order_id','payment_sequential'])\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Increase readibility of the categorical data\n",
    "    df.loc[:, 'payment_type'] = df['payment_type'].str.replace('_',' ').str.title()\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    # Step 5: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['payment_sequential'] = df['payment_sequential'].astype('int8')\n",
    "    df['payment_type'] = df['payment_type'].astype('category')\n",
    "    df['payment_installments'] = df['payment_installments'].astype('int8')\n",
    "    df['payment_value'] = df['payment_value'].apply(Decimal)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_order_payments_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'order_payments'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fc3849-00ce-4ac5-9be8-9a43366a0dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Data Cleaning for \"order_reviews\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues \n",
    "- retained review_score as 'int64' instead oa 'category' in case we would like to do mathematical operations such as applying some statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c437a4fd-de2b-432f-bd11-da0270a48452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_order_reviews_dataset.csv written successfully!\n",
      "Data written to order_reviews table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_order_reviews_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `order_id`+'review_id' if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['order_id','review_id'])\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 4: Convert columns to appropriate data types\n",
    "    df['review_id'] = df['review_id'].astype('string')\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['review_score'] = df['review_score'].astype('int64')\n",
    "    df['review_comment_title'] = df['review_comment_title'].astype('string')\n",
    "    df['review_comment_message'] = df['review_comment_message'].astype('string')\n",
    "    df['review_creation_date'] = pd.to_datetime(df['review_creation_date'])\n",
    "    df['review_answer_timestamp'] = pd.to_datetime(df['review_answer_timestamp']).dt.date\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_order_reviews_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'order_reviews'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a41aa09-6952-4237-b609-f9ab7ef1f8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Data Cleaning for \"orders\"\n",
    "- convert columns to appropriate data types\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "\n",
    "Issues \n",
    "- if set to_datetime first, .fillna later is ok, but if .fillna first, to_datetime later is not ok\n",
    "- however if set to 'category' first, .fillna later is not ok\n",
    "- hence the solution here is to set as 'string' first, then .fillna, then set as 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5c392b-9af3-445b-9a78-f85b89b466ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_orders_dataset.csv written successfully!\n",
      "Data written to orders table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_orders_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Convert columns to appropriate data types\n",
    "    df['order_id'] = df['order_id'].astype('string')\n",
    "    df['customer_id'] = df['customer_id'].astype('string')\n",
    "    df['order_status'] = df['order_status'].astype('string')\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "    df['order_delivered_carrier_date'] = pd.to_datetime(df['order_delivered_carrier_date'])\n",
    "    df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "    df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])\n",
    "\n",
    "    # Step 2: Remove duplicates in `order_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='order_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Increase readibility of the categorical data\n",
    "    df['order_status'] = df['order_status'].str.title()\n",
    "\n",
    "    # Step 5: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A') \n",
    "\n",
    "    # Step 6: Convert 'order_status' to category\n",
    "    df['order_status'] = df['order_status'].astype('category')\n",
    "\n",
    "    # Step 7: Add customer_unique_id column to orders dataset, then drop the customer_id column        \n",
    "    file_name = 'olist_customers_dataset.csv'\n",
    "    blob_name = folder + file_name \n",
    "    blob_client = container_client.get_blob_client(blob_name) \n",
    "    blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "    # Use StringIO to read the content into a Pandas DataFrame \n",
    "    customers = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "    \n",
    "    df = df.merge(customers[['customer_id', 'customer_unique_id']], left_on='customer_id', right_on='customer_id', how='left')\n",
    "    df = df.drop(columns=['customer_id'])\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_orders_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'orders'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df87d5b6-2f59-4903-8303-5ed8ec2ba4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Data Cleaning for \"products\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values in string columns\n",
    "- handling NaN or inf in integer columns\n",
    "- convert columns to appropriate data types\n",
    "\n",
    "Issues\n",
    "- issue is that .fillna has to come before .astype('category'), and some columns had null values which cannot be set .astype('int64')\n",
    "- hence, step 3 & 4 separated into str columns and int columns, for filling with 'N/A' or '0' respectively\n",
    "- subsequently, step 5 conversion into 'string', 'category' and 'int64' data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094e069c-62f6-4fa6-942b-64469b2111dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_products_dataset.csv written successfully!\n",
      "Data written to products table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_products_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df): \n",
    "    # Step 1: Translate Portugese category names to English\n",
    "    products_df = df\n",
    "\n",
    "    file_name = 'product_category_name_translation.csv'\n",
    "    blob_name = folder + file_name \n",
    "    blob_client = container_client.get_blob_client(blob_name) \n",
    "    blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "    # Use StringIO to read the content into a Pandas DataFrame \n",
    "    translation_df = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "    # Improve readibility of the text\n",
    "    translation_df['product_category_name_english'] = translation_df['product_category_name_english'].str.replace('_',' ').str.title()\n",
    "    \n",
    "    # Fix typo errors\n",
    "    replacements = {\n",
    "    'Costruction Tools Garden': 'Construction Tools Garden',\n",
    "    'Home Confort': 'Home Comfort',\n",
    "    'Costruction Tools Tools': 'Construction Tools Tools',\n",
    "    'Fashio Female Clothing': 'Fashion Female Clothing',\n",
    "    'Fashion Childrens Clothes': 'Fashion Children Clothes',\n",
    "    'Dvd': 'DVD',\n",
    "    'Cd': 'CD'}\n",
    "    translation_df['product_category_name_english'] = translation_df['product_category_name_english'].replace(replacements, regex=True)\n",
    "    \n",
    "    merged_df = products_df.merge(translation_df, on='product_category_name', how='left')\n",
    "    merged_df.insert(1, 'product_category_name_english_merged', merged_df['product_category_name_english'])\n",
    "    merged_dropped_df = merged_df.drop(['product_category_name', 'product_category_name_english'], axis=1)  \n",
    "    df = merged_dropped_df\n",
    "    \n",
    "    # Step 2: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='product_id')\n",
    "    \n",
    "    # Step 3: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\" in the string columns:\n",
    "    str_columns = ['product_id', 'product_category_name_english_merged']\n",
    "    df[str_columns] = df[str_columns].fillna('N/A')\n",
    "\n",
    "    # Step 5: Replace NaN or inf with 0 in the integer columns:\n",
    "    int_columns = ['product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "                   'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
    "    df[int_columns] = df[int_columns].replace([np.nan, np.inf, -np.inf], 0)\n",
    "\n",
    "    # Step 6: Convert columns to appropriate data types\n",
    "    df['product_id'] = df['product_id'].astype('string')\n",
    "    df['product_category_name_english_merged'] = df['product_category_name_english_merged'].astype('category')\n",
    "    df['product_name_lenght'] = df['product_name_lenght'].astype('int64')  \n",
    "    df['product_description_lenght'] = df['product_description_lenght'].astype('int64')  \n",
    "    df['product_photos_qty'] = df['product_photos_qty'].astype('int64')  \n",
    "    df['product_weight_g'] = df['product_weight_g'].astype('int64') \n",
    "    df['product_length_cm'] = df['product_length_cm'].astype('int64') \n",
    "    df['product_height_cm'] = df['product_height_cm'].astype('int64') \n",
    "    df['product_width_cm'] = df['product_width_cm'].astype('int64')\n",
    "\n",
    "    # Step 7: Rename columns 'product_name_lenght' and 'product_description_lenght' to the correct spelling of 'length'\n",
    "    df = df.rename(columns={'product_category_name_english_merged':'product_category','product_name_lenght': 'product_name_length', 'product_description_lenght' : 'product_description_length'})\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_products_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'products'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e13f6d8-ef50-40c9-a160-bc7687ae129b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Data Cleaning for \"sellers\"\n",
    "- removed duplicates\n",
    "- strip leading/trailing whitespace\n",
    "- handling null values\n",
    "- convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38e3b38-d9c8-42cd-9bca-bbd17bb93775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset cleaned_olist_sellers_dataset.csv written successfully!\n",
      "Data written to sellers table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_name = 'olist_sellers_dataset.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Step 1: Remove duplicates in `product_id` if any, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset='seller_id')\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace from all string columns\n",
    "    str_columns = df.select_dtypes(include='string').columns\n",
    "    df[str_columns] = df[str_columns].apply(lambda col: col.str.strip())\n",
    "\n",
    "    # Step 3: Increase readibility of the categorical data\n",
    "    df['seller_city'] = df['seller_city'].str.title()\n",
    "\n",
    "    # Step 4: Replace NULL values with the string \"N/A\":\n",
    "    df = df.fillna('N/A')\n",
    "\n",
    "    # Step 5: Convert columns to appropriate data types\n",
    "    df['seller_id'] = df['seller_id'].astype('string')\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].astype('string')\n",
    "    df['seller_city'] = df['seller_city'].astype('category') \n",
    "    df['seller_state'] = df['seller_state'].astype('category')\n",
    "\n",
    "    # Step 6: Normalize `customer_zip_code_prefix` to ensure all are 5 characters\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].str.zfill(5)\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"cleaned_olist_sellers_dataset.csv\"\n",
    "\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'sellers'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47313f65-4160-41c8-baa3-c870309fb236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset states_lookup.csv written successfully!\n",
      "Data written to states table in Azure PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# State names look up table\n",
    "# Data from other Kaggler dataset\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'misc/states.csv'\n",
    "blob_name = folder + file_name \n",
    "blob_client = container_client.get_blob_client(blob_name) \n",
    "blob_content = blob_client.download_blob().readall() \n",
    "\n",
    "# Use StringIO to read the content into a Pandas DataFrame \n",
    "data = pd.read_csv(StringIO(blob_content.decode('utf-8')))\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "\n",
    "    df = df[['UF', 'State']]  # Keep state codes and names only\n",
    "    df = df.rename(columns={'UF': 'code', 'State' : 'state'})  # Rename UF and State to code and state respectively\n",
    "    \n",
    "    # Normalise accented strings to normal alphabet strings\n",
    "    df['state'] = df['state'].apply(lambda x: unidecode(x))\n",
    "    \n",
    "    # Copy converted proper name to title case \n",
    "    df['state'] = df['state'].str.title()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "# Write cleaned data to Blob Storage cleaned folder\n",
    "cleaned_file = \"states_lookup.csv\"\n",
    "container_client.get_blob_client(f\"{folder}{cleaned_folder}{cleaned_file}\").upload_blob(cleaned_data.to_csv(index=False), overwrite=True)\n",
    "print(f\"Cleaned dataset {cleaned_file} written successfully!\")\n",
    "\n",
    "# Write cleaned data to PostgresSQL server\n",
    "table = 'states'\n",
    "cleaned_data.to_sql(table, engine, if_exists='replace', index=False)\n",
    "print(f\"Data written to {table} table in Azure PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65d77e6-f1dd-41d9-880c-f0d6ae37ba88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "v3.1 bricks olist (No Sparks)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
